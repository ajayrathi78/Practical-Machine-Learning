{"name":"Practical-machine-learning","tagline":"","body":"---\r\ntitle: \"Practical Machine Learning Project\"\r\nauthor: \"Ajay Rathi\"\r\ndate: \"23 August 2015\"\r\noutput: html_document\r\n---------\r\nSYNOPSIS\r\n------------------\r\nSource:\r\n\r\nVelloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.\r\n\r\nGoal of this project is to “predict the manner in which trainers did the exercise.”\r\n\r\nFurther report should describe:\r\n\r\n“how you built your model”\r\n“how you used cross validation”\r\n“what you think the expected out of sample error is”\r\n“why you made the choices you did”\r\nUltimately, the prediction model is to be run on the test data to predict the outcome of 20 different test cases.\r\n\r\nFirst, though, I'll load the appropriate packages and set the seed for reproduceable results.\r\n\r\n```{r}\r\nlibrary(AppliedPredictiveModeling)\r\nlibrary(caret)\r\nlibrary(rattle)\r\nlibrary(rpart.plot)\r\nlibrary(randomForest)\r\n```\r\nQUESTION\r\n----------------\r\nIn the aforementioned study, six participants participated in a dumbell lifting exercise five different ways. The five ways, as described in the study, were “exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes.”\r\n\r\nBy processing data gathered from accelerometers on the belt, forearm, arm, and dumbell of the participants in a machine learning algorithm, the question is can the appropriate activity quality (class A-E) be predicted?\r\n\r\nINPUT DATA\r\n-----------\r\nThe first step is to import the data and to verify that the training data and the test data are identical.\r\n\r\n```{r}\r\n# Download data.\r\n#url_raw_training <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\nfile_dest_training <- \"pml-training.csv\"\r\n#download.file(url=url_raw_training, destfile=file_dest_training, method=\"curl\")\r\n#url_raw_testing <- \"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\nfile_dest_testing <- \"pml-testing.csv\"\r\n#download.file(url=url_raw_testing, destfile=file_dest_testing, method=\"curl\")\r\n\r\n# Import the data treating empty values as NA.\r\ndf_training <- read.csv(file_dest_training, na.strings=c(\"NA\",\"\"), header=TRUE)\r\ncolnames_train <- colnames(df_training)\r\ndf_testing <- read.csv(file_dest_testing, na.strings=c(\"NA\",\"\"), header=TRUE)\r\ncolnames_test <- colnames(df_testing)\r\n\r\n# Verify that the column names (excluding classe and problem_id) are identical in the training and test set.\r\nall.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])\r\n```\r\nFEATURES\r\n---------\r\nHaving verified that the schema of both the training and testing sets are identical (excluding the final column representing the A-E class), I decided to eliminate both NA columns and other extraneous columns.\r\n\r\n```{r}\r\n# Count the number of non-NAs in each col.\r\nnonNAs <- function(x) {\r\n    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))\r\n}\r\n\r\n# Build vector of missing data or NA columns to drop.\r\ncolcnts <- nonNAs(df_training)\r\ndrops <- c()\r\nfor (cnt in 1:length(colcnts)) {\r\n    if (colcnts[cnt] < nrow(df_training)) {\r\n        drops <- c(drops, colnames_train[cnt])\r\n    }\r\n}\r\n\r\n# Drop NA data and the first 7 columns as they're unnecessary for predicting.\r\ndf_training <- df_training[,!(names(df_training) %in% drops)]\r\ndf_training <- df_training[,8:length(colnames(df_training))]\r\n\r\ndf_testing <- df_testing[,!(names(df_testing) %in% drops)]\r\ndf_testing <- df_testing[,8:length(colnames(df_testing))]\r\n\r\n# Show remaining columns.\r\ncolnames(df_training)\r\n```\r\n```{r}\r\ncolnames(df_testing)\r\n```\r\n\r\nFirst, check for covariates that have virtually no variablility.\r\n\r\n```{r}\r\nnsv <- nearZeroVar(df_training, saveMetrics=TRUE)\r\nnsv\r\n```\r\nGiven that all of the near zero variance variables (nsv) are FALSE, there's no need to eliminate any covariates due to lack of variablility.\r\n\r\nALGORITHM\r\n-----------\r\nWe were provided with a large training set (19,622 entries) and a small testing set (20 entries). Instead of performing the algorithm on the entire training set, as it would be time consuming and wouldn't allow for an attempt on a testing set, I chose to divide the given training set into four roughly equal sets, each of which was then split into a training set (comprising 60% of the entries) and a testing set (comprising 40% of the entries).\r\n\r\n```{r}\r\n# Divide the given training set into 4 roughly equal sets.\r\nset.seed(666)\r\nids_small <- createDataPartition(y=df_training$classe, p=0.25, list=FALSE)\r\ndf_small1 <- df_training[ids_small,]\r\ndf_remainder <- df_training[-ids_small,]\r\nset.seed(666)\r\nids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)\r\ndf_small2 <- df_remainder[ids_small,]\r\ndf_remainder <- df_remainder[-ids_small,]\r\nset.seed(666)\r\nids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)\r\ndf_small3 <- df_remainder[ids_small,]\r\ndf_small4 <- df_remainder[-ids_small,]\r\n# Divide each of these 4 sets into training (60%) and test (40%) sets.\r\nset.seed(666)\r\ninTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)\r\ndf_small_training1 <- df_small1[inTrain,]\r\ndf_small_testing1 <- df_small1[-inTrain,]\r\nset.seed(666)\r\ninTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)\r\ndf_small_training2 <- df_small2[inTrain,]\r\ndf_small_testing2 <- df_small2[-inTrain,]\r\nset.seed(666)\r\ninTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)\r\ndf_small_training3 <- df_small3[inTrain,]\r\ndf_small_testing3 <- df_small3[-inTrain,]\r\nset.seed(666)\r\ninTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)\r\ndf_small_training4 <- df_small4[inTrain,]\r\ndf_small_testing4 <- df_small4[-inTrain,]\r\n```\r\nPARAMETERS\r\n----------\r\nI decided to try classification trees “out of the box” and then introduce preprocessing and cross validation.\r\n\r\nWhile I also considered applying “out of the box” random forest models, some of the horror stories contributed to the coursera discussion forums regarding the lengthy processing times for random forest models convinced me to only attempt random forests with cross validation and, possibly, preprocessing.\r\n\r\nEVALUATION\r\n----------------\r\nClassification Tree\r\n\r\nFirst, the “out of the box” classification tree:\r\n\r\n```{r}\r\n# Train on training set 1 of 4 with no extra features.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ ., data = df_small_training1, method=\"rpart\")\r\nprint(modFit, digits=3)\r\n```\r\n\r\n```{r}\r\nprint(modFit$finalModel, digits=3)\r\n```\r\n```{r}\r\nfancyRpartPlot(modFit$finalModel)\r\n```\r\n\r\n```{r}\r\n# Run against testing set 1 of 4 with no extra features.\r\npredictions <- predict(modFit, newdata=df_small_testing1)\r\nprint(confusionMatrix(predictions, df_small_testing1$classe), digits=4)\r\n```\r\n\r\nlow accuracy rate (0.5584)\r\n\r\n```{r}\r\n# Train on training set 1 of 4 with only preprocessing.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ .,  preProcess=c(\"center\", \"scale\"), data = df_small_training1, method=\"rpart\")\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Train on training set 1 of 4 with only cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ .,  trControl=trainControl(method = \"cv\", number = 4), data = df_small_training1, method=\"rpart\")\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Train on training set 1 of 4 with both preprocessing and cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ .,  preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data = df_small_training1, method=\"rpart\")\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Run against testing set 1 of 4 with both preprocessing and cross validation.\r\npredictions <- predict(modFit, newdata=df_small_testing1)\r\nprint(confusionMatrix(predictions, df_small_testing1$classe), digits=4)\r\n```\r\n\r\nThe impact of incorporating both preprocessing and cross validation appeared to show some minimal improvement (accuracy rate rose from 0.531 to 0.552 against training sets). However, when run against the corresponding testing set, the accuracy rate was identical (0.5584) for both the “out of the box” and the preprocessing/cross validation methods.\r\n\r\nRandom Forest\r\n--------------\r\nFirst I decided to assess the impact/value of including preprocessing.\r\n\r\n```{r}\r\n# Train on training set 1 of 4 with only cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ ., method=\"rf\", trControl=trainControl(method = \"cv\", number = 4), data=df_small_training1)\r\nprint(modFit, digits=3)\r\n```\r\n\r\n```{r}\r\n# Run against testing set 1 of 4.\r\npredictions <- predict(modFit, newdata=df_small_testing1)\r\nprint(confusionMatrix(predictions, df_small_testing1$classe), digits=4)\r\n```\r\n```{r}\r\n# Run against 20 testing set\r\nprint(predict(modFit, newdata=df_testing))\r\n```\r\n\r\n```{r}\r\n# Train on training set 1 of 4 with only both preprocessing and cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training1$classe ~ ., method=\"rf\", preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data=df_small_training1)\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Run against testing set 1 of 4.\r\npredictions <- predict(modFit, newdata=df_small_testing1)\r\nprint(confusionMatrix(predictions, df_small_testing1$classe), digits=4)\r\n```\r\n\r\n```{r}\r\n# Run against 20 testing\r\nprint(predict(modFit, newdata=df_testing))\r\n```\r\n\r\nPreprocessing actually lowered the accuracy rate from 0.955 to 0.954 against the training set. However, when run against the corresponding set, the accuracy rate rose from 0.9689 to 0.9714 with the addition of preprocessing. Thus I decided to apply both preprocessing and cross validation to the remaining 3 data sets.\r\n\r\n```{r}\r\n# Train on training set 2 of 4 with only cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training2$classe ~ ., method=\"rf\", preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data=df_small_training2)\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Run against testing set 2 of 4.\r\npredictions <- predict(modFit, newdata=df_small_testing2)\r\nprint(confusionMatrix(predictions, df_small_testing2$classe), digits=4)\r\n```\r\n```{r}\r\n# Run against 20 testing set\r\nprint(predict(modFit, newdata=df_testing))\r\n```\r\n```{r}\r\n# Train on training set 3 of 4 with only cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training3$classe ~ ., method=\"rf\", preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data=df_small_training3)\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Run against testing set 3 of 4.\r\npredictions <- predict(modFit, newdata=df_small_testing3)\r\nprint(confusionMatrix(predictions, df_small_testing3$classe), digits=4)\r\n```\r\n```{r}\r\n# Run against 20 testing set\r\nprint(predict(modFit, newdata=df_testing))\r\n```\r\n```{r}\r\n# Train on training set 4 of 4 with only cross validation.\r\nset.seed(666)\r\nmodFit <- train(df_small_training4$classe ~ ., method=\"rf\", preProcess=c(\"center\", \"scale\"), trControl=trainControl(method = \"cv\", number = 4), data=df_small_training4)\r\nprint(modFit, digits=3)\r\n```\r\n```{r}\r\n# Run against testing set 4 of 4.\r\npredictions <- predict(modFit, newdata=df_small_testing4)\r\nprint(confusionMatrix(predictions, df_small_testing4$classe), digits=4)\r\n```\r\n```{r}\r\n# Run against 20 testing set provided by Professor Leek.\r\nprint(predict(modFit, newdata=df_testing))\r\n```\r\nCONCLUSION\r\n---------------------\r\nI received three separate predictions by appling the 4 models against the actual 20 item training set:\r\n\r\nA) Accuracy Rate 0.0286 Predictions: B A A A A E D B A A B C B A E E A B B B\r\n\r\nB) Accuracy Rates 0.0366 and 0.0345 Predictions: B A B A A E D B A A B C B A E E A B B B\r\n\r\nC) Accuracy Rate 0.0437 Predictions: B A B A A E D D A A B C B A E E A B B B\r\n\r\nSince Professor Leek is allowing 2 submissions for each problem, I decided to attempt with the two most likely prediction sets: option A and option B.\r\n\r\nSince options A and B above only differed for item 3 (A for option A, B for option B), I subimitted one value for problems 1-2 and 4-20, while I submitted two values for problem 3. For problem 3, I was expecting the automated grader to tell me which answer (A or B) was correct, but instead the grader simply told me I had a correct answer. All other answers were also correct, resulting in a score of 100%.\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}